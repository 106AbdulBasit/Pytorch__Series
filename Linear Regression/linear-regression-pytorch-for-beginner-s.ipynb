{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1 style=\"font-family:Times New Roman;\"> <center>Linear Regression : EDA Pytorch For Beginner's</center> </h1>\n<p><center style=\"color:#159364; font-family:Times New Roman;font-size:30px;\">‚ÄúDevelop a passion for learning. If you do, you will never cease to grow.‚Äù ‚Äì Anthony J. D‚ÄôAngelo</center></p>","metadata":{}},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-info\" style=\"font-size:30px; font-family:Times New Roman; line-height: 1.7em;\">\n    üìå &nbsp; Please vote up if you like this. I would like to request you to vote up and share this with fellow kaggler's </div>","metadata":{}},{"cell_type":"markdown","source":"## Table Of Contents\n1 [Introduction](#1)\n\n2 [Importing Libaries](#2)\n\n3 [Importing Data Set](#3)\n\n4 [EDA](#4)\n\n5 [Preparing Data Set](#5)\n\n6 [Making Mode](#6)\n\n7 [Training](#7)\n\n8 [Plotiing The Resuly](#8)\n","metadata":{}},{"cell_type":"markdown","source":"<h1  id=\"1\" style=\"color:black;\n           display:fill;\n           border-radius:5px;\n           background-color:#FFFFFF;\n           font-size:350%;\n           font-family:Times New Roman;\n           letter-spacing:0.5px\"> 1 Introduction </h1>","metadata":{}},{"cell_type":"markdown","source":"<span style=\"font-family:Times New Roman; font-size:25px; font-weight: bold;\">Linear Regression. </span>\n\n<span style=\"font-family:Times New Roman; font-size:18px;\">To get our feet wet, we'll start off by looking at the problem of regression. This is the task of predicting a real valued target  given a data point . In linear regression, the simplest and still perhaps the most useful approach, we assume that prediction can be expressed as a linear combination of the input features (thus giving the name linear regression): </span>\n\n<span style=\"font-family:Times New Roman; font-size:18px;\">A linear regression line has an equation of the form Y = a + bX, where X is the explanatory variable and Y is the dependent variable. The slope of the line is b, and a is the intercept (the value of y when x = 0). </span>","metadata":{}},{"cell_type":"markdown","source":"<span style=\"font-family:Times New Roman; font-size:25px; font-weight: bold;\">Mean Squared Error. </span>\n\n<span style=\"font-family:Times New Roman; font-size:18px;\"> Generally, we will define a loss function that says how far are our predictions from the correct answers. For the classical case of linear regression, we usually focus on the squared error. Specifically, our loss will be the sum, over all examples, of the squared error \n on each:  </span>\n ","metadata":{}},{"cell_type":"markdown","source":"![MSE](https://raw.githubusercontent.com/106AbdulBasit/Pytorch__Series/main/Linear%20Regression/MSE.PNG)","metadata":{}},{"cell_type":"markdown","source":"<h1  id=\"2\" style=\"color:black;\n           display:fill;\n           border-radius:5px;\n           background-color:#FFFFFF;\n           font-size:350%;\n           font-family:Times New Roman;\n           letter-spacing:0.5px\"> 2 Importing some Libraries </h1>","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd \nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')\nimport matplotlib","metadata":{"execution":{"iopub.status.busy":"2022-07-17T08:06:17.693330Z","iopub.execute_input":"2022-07-17T08:06:17.693766Z","iopub.status.idle":"2022-07-17T08:06:17.700944Z","shell.execute_reply.started":"2022-07-17T08:06:17.693733Z","shell.execute_reply":"2022-07-17T08:06:17.699564Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"<h1  id=\"3\" style=\"color:black;\n           display:fill;\n           border-radius:5px;\n           background-color:#FFFFFF;\n           font-size:350%;\n           font-family:Times New Roman;\n           letter-spacing:0.5px\"> 3 Importing Data Set </h1>","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/random-linear-regression/train.csv')\ntest = pd.read_csv('/kaggle/input/random-linear-regression/test.csv')\n\n#dropna Remove missing values.\n\ntrain = train.dropna()\ntest = test.dropna()\n# \n# spliting into labels and features\nx_train = train.iloc[:,0].values.reshape(-1,1)\ny_train = train.iloc[:,1].values.reshape(-1,1)\n\nx_test = test.iloc[:,0].values.reshape(-1,1)\ny_test = test.iloc[:,1].values.reshape(-1,1)","metadata":{"execution":{"iopub.status.busy":"2022-07-17T08:05:14.181774Z","iopub.execute_input":"2022-07-17T08:05:14.182478Z","iopub.status.idle":"2022-07-17T08:05:14.226587Z","shell.execute_reply.started":"2022-07-17T08:05:14.182440Z","shell.execute_reply":"2022-07-17T08:05:14.225415Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"<h1  id=\"4\" style=\"color:black;\n           display:fill;\n           border-radius:5px;\n           background-color:#FFFFFF;\n           font-size:350%;\n           font-family:Times New Roman;\n           letter-spacing:0.5px\"> 4 Explotary Data Anaylsis </h1>","metadata":{}},{"cell_type":"code","source":"print(train)","metadata":{"execution":{"iopub.status.busy":"2022-07-17T08:05:19.262447Z","iopub.execute_input":"2022-07-17T08:05:19.262871Z","iopub.status.idle":"2022-07-17T08:05:19.278469Z","shell.execute_reply.started":"2022-07-17T08:05:19.262834Z","shell.execute_reply":"2022-07-17T08:05:19.277057Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(test)","metadata":{"execution":{"iopub.status.busy":"2022-07-17T08:05:25.539442Z","iopub.execute_input":"2022-07-17T08:05:25.540084Z","iopub.status.idle":"2022-07-17T08:05:25.548561Z","shell.execute_reply.started":"2022-07-17T08:05:25.540046Z","shell.execute_reply":"2022-07-17T08:05:25.547532Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"train.head()\nprint(f'Shape of the train set {train.size} \\n train columns {train.columns.values}')","metadata":{"execution":{"iopub.status.busy":"2022-07-17T08:05:29.101879Z","iopub.execute_input":"2022-07-17T08:05:29.102315Z","iopub.status.idle":"2022-07-17T08:05:29.109616Z","shell.execute_reply.started":"2022-07-17T08:05:29.102282Z","shell.execute_reply":"2022-07-17T08:05:29.108207Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"test.head()\nprint(f'Shape of the test set {test.size} \\n train columns {test.columns.values}')","metadata":{"execution":{"iopub.status.busy":"2022-07-17T08:05:34.953659Z","iopub.execute_input":"2022-07-17T08:05:34.954126Z","iopub.status.idle":"2022-07-17T08:05:34.961934Z","shell.execute_reply.started":"2022-07-17T08:05:34.954082Z","shell.execute_reply":"2022-07-17T08:05:34.960755Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"train.describe().T","metadata":{"execution":{"iopub.status.busy":"2022-07-17T08:05:40.268853Z","iopub.execute_input":"2022-07-17T08:05:40.269362Z","iopub.status.idle":"2022-07-17T08:05:40.315564Z","shell.execute_reply.started":"2022-07-17T08:05:40.269317Z","shell.execute_reply":"2022-07-17T08:05:40.314729Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"test.describe().T","metadata":{"execution":{"iopub.status.busy":"2022-07-17T08:05:45.741499Z","iopub.execute_input":"2022-07-17T08:05:45.741963Z","iopub.status.idle":"2022-07-17T08:05:45.766042Z","shell.execute_reply.started":"2022-07-17T08:05:45.741928Z","shell.execute_reply":"2022-07-17T08:05:45.765226Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"<h1  id=\"5\" style=\"color:black;\n           display:fill;\n           border-radius:5px;\n           background-color:#FFFFFF;\n           font-size:350%;\n           font-family:Times New Roman;\n           letter-spacing:0.5px\"> 5 Peparing Data set </h1>","metadata":{}},{"cell_type":"markdown","source":"\n\n<span style=\"font-family:Times New Roman; font-size:18px;\">In Numpy, you may have an array that has three dimensions, right? That is, technically speaking, a tensor. </span>\n\n\n<span style=\"font-family:Times New Roman; font-size:18px;\">\nA scalar (a single number) has zero dimensions, a vector has one dimension, a matrix has two dimensions and a tensor has three or more dimensions. That‚Äôs it!. </span>\n\n\n\n<span style=\"font-family:Times New Roman; font-size:18px;\"> But, to keep things simple, it is commonplace to call vectors and matrices tensors as well ‚Äî so, from now on, everything is either a scalar or a tensor. </span>\n\n\n","metadata":{}},{"cell_type":"markdown","source":"![tensor](https://raw.githubusercontent.com/106AbdulBasit/Pytorch__Series/main/Linear%20Regression/Tensors.PNG)","metadata":{}},{"cell_type":"code","source":"# random generated\nx_train_tensor = torch.from_numpy(x_train).float()\ny_train_tensor = torch.from_numpy(y_train).float()\n\nx_test_tensor = torch.from_numpy(x_test).float()\ny_test_tensor = torch.from_numpy(y_test).float()\n\n\nfrom torch.utils.data import TensorDataset\nfrom torch.utils.data import DataLoader\n\n\ndataset_train = TensorDataset(x_train_tensor, y_train_tensor)\ndataset_test = TensorDataset(x_test_tensor, y_test_tensor)\n\n# how assign\ntrain_loader = DataLoader(dataset=dataset_train, batch_size=50)\ntest_loader = DataLoader(dataset=dataset_test, batch_size=60)","metadata":{"execution":{"iopub.status.busy":"2022-07-17T08:06:25.825509Z","iopub.execute_input":"2022-07-17T08:06:25.825941Z","iopub.status.idle":"2022-07-17T08:06:25.833808Z","shell.execute_reply.started":"2022-07-17T08:06:25.825899Z","shell.execute_reply":"2022-07-17T08:06:25.832361Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1  id=\"6\" style=\"color:black;\n           display:fill;\n           border-radius:5px;\n           background-color:#FFFFFF;\n           font-size:350%;\n           font-family:Times New Roman;\n           letter-spacing:0.5px\"> 6 Making The Model </h1>\n           \n\n\n<span style=\"font-family:Times New Roman; font-size:18px;\">This is generic model Function. If you want to change the optimizer or lossfunction or model you can  just  pass those function name to this generic function. </span>","metadata":{}},{"cell_type":"code","source":"def make_train_step(model, loss_fn, optimizer):\n    # builds & returns the function that will be called inside the loop\n    def train_step(x, y):\n        model.train()\n        yhat = model(x)\n        loss = loss_fn(y, yhat)\n        loss.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n        return loss.item()\n    return train_step","metadata":{"execution":{"iopub.status.busy":"2022-07-17T08:06:35.132414Z","iopub.execute_input":"2022-07-17T08:06:35.132873Z","iopub.status.idle":"2022-07-17T08:06:35.138902Z","shell.execute_reply.started":"2022-07-17T08:06:35.132836Z","shell.execute_reply":"2022-07-17T08:06:35.138057Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"<h4   style=\"color:black;\n           display:fill;\n           border-radius:5px;\n           background-color:#FFFFFF;\n           font-size:350%;\n           font-family:Times New Roman;\n           letter-spacing:0.5px\"> Loss and Optimizer </h4>","metadata":{}},{"cell_type":"markdown","source":"\n\n<span style=\"font-family:Times New Roman; font-size:18px;\"> Train a model means making it better and better over the course of a period of training. But in order for this goal to make any sense at all, we first need to define what better means in the first place. In this case, we'll use the MSE between our prediction and the true value. Instead of writing our own loss function we‚Äôre just going to access squared error by instantiating nn.MESLoss. </span>\n\n\n\n\n\n<span style=\"font-family:Times New Roman; font-size:18px;\"> It turns out that linear regression actually has a closed-form solution. However, most interesting models that we'll care about cannot be solved analytically. So we'll solve this problem by stochastic gradient descent. At each step, we'll estimate the gradient of the loss with respect to our weights, using one batch randomly drawn from our dataset. Then, we'll update our parameters a small amount in the direction that reduces the loss. The size of the step is determined by the learning rate lr. </span>\n\n\n","metadata":{}},{"cell_type":"code","source":"\n\ndevice = 'cpu'\n\n# hyperparameters\nlr = 1e-6\nn_epochs = 1000\n\nfrom sklearn.metrics import r2_score\n\n# loss function & optimizer\nmodel = nn.Sequential(nn.Linear(1, 1)).to(device)\nloss_fn = nn.MSELoss(reduction='mean')\noptimizer = optim.SGD(model.parameters(), lr=lr)","metadata":{"execution":{"iopub.status.busy":"2022-07-17T08:06:47.103178Z","iopub.execute_input":"2022-07-17T08:06:47.103629Z","iopub.status.idle":"2022-07-17T08:06:47.230903Z","shell.execute_reply.started":"2022-07-17T08:06:47.103593Z","shell.execute_reply":"2022-07-17T08:06:47.229437Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"<h1  id=\"7\" style=\"color:black;\n           display:fill;\n           border-radius:5px;\n           background-color:#FFFFFF;\n           font-size:350%;\n           font-family:Times New Roman;\n           letter-spacing:0.5px\">  7 Training and Evaluation </h1>","metadata":{}},{"cell_type":"markdown","source":"Now that we have all the pieces, we just need to wire them together by writing a training loop. First we'll define epochs, the number of passes to make over the dataset. Then for each pass, we'll iterate through train_data, grabbing batches of examples and their corresponding labels.\n\nFor each batch, we'll go through the following ritual:\n\nGenerate predictions (yhat) and the loss (loss) by executing a forward pass through the network.\nCalculate gradients by making a backwards pass through the network (loss.backward()).\nUpdate the model parameters by invoking our SGD optimizer, use optimizer.step().","metadata":{}},{"cell_type":"code","source":"\n# training step\ntrain_step = make_train_step(model, loss_fn, optimizer)\ntraining_losses = []\ntest_losses = []\naccuracies = []\nfor epoch in range(n_epochs):\n    batch_losses = []\n    for nbatch, (x_batch, y_batch) in enumerate(train_loader):\n        #print(nbatch)\n        x_batch = x_batch.to(device)\n        y_batch = y_batch.to(device)\n        loss = train_step(x_batch, y_batch)\n        batch_losses.append(loss)\n    training_loss = np.mean(batch_losses)\n    training_losses.append(training_loss)\n    # \n    ## here evaluation \n    with torch.no_grad():\n        # batches for test\n        val_losses = []\n        accs = []\n        for x_test, y_test in test_loader:\n            x_test = x_test.to(device)\n            y_test = y_test.to(device)\n            # \n            model.eval()\n            yhat = model(x_test)\n            test_loss = loss_fn(y_test, yhat)\n            test_losses.append(test_loss)\n            acc = r2_score(y_test, yhat)\n            accs.append(acc)\n        test_loss = np.mean(test_losses)\n        test_losses.append(test_loss)\n        acc_batch = np.mean(accs)\n        accuracies.append(acc_batch)\n    if epoch % 50 == 0:\n        print(f'epoch {epoch+1} | Training loss: {training_loss:.4f} | Test loss: {test_loss:.4f} | R2 {acc_batch:.4f}')","metadata":{"execution":{"iopub.status.busy":"2022-07-17T08:06:59.007960Z","iopub.execute_input":"2022-07-17T08:06:59.009178Z","iopub.status.idle":"2022-07-17T08:07:19.809880Z","shell.execute_reply.started":"2022-07-17T08:06:59.009092Z","shell.execute_reply":"2022-07-17T08:07:19.808625Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"<h1  id=\"8\" style=\"color:black;\n           display:fill;\n           border-radius:5px;\n           background-color:#FFFFFF;\n           font-size:350%;\n           font-family:Times New Roman;\n           letter-spacing:0.5px\"> Plotting the Result </h1>","metadata":{}},{"cell_type":"code","source":"print(model.state_dict())\nplt.style.use('seaborn')\nplt.figure(figsize=(8,6)) \nplt.scatter(x_test, y_test, s=10)\nplt.plot(x_test,model(x_test).detach().numpy(), 'r', linewidth=0.4)","metadata":{"execution":{"iopub.status.busy":"2022-07-17T08:07:27.056535Z","iopub.execute_input":"2022-07-17T08:07:27.056902Z","iopub.status.idle":"2022-07-17T08:07:27.294633Z","shell.execute_reply.started":"2022-07-17T08:07:27.056873Z","shell.execute_reply":"2022-07-17T08:07:27.293332Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"\n\n<span style=\"font-family:Times New Roman; font-size:18px;\">For Reference  check the below link </span>\n[Understanding Pytorch](https://towardsdatascience.com/understanding-pytorch-with-an-example-a-step-by-step-tutorial-81fc5f8c4e8e#3a3f)","metadata":{}},{"cell_type":"markdown","source":"<h1  id=\"9\" style=\"color:black;\n           display:fill;\n           border-radius:5px;\n           background-color:#FFFFFF;\n           font-size:350%;\n           font-family:Times New Roman;\n           letter-spacing:0.5px\"> What's Next </h1>\n   \n\n\n<span style=\"font-family:Times New Roman; font-size:18px;\"> Convolutional Neural Network Basic in Tensor Flow Dog vs cat. </span>\n\n[Kaggle Link](https://www.kaggle.com/code/abdulbasitniazi/vs-how-cnn-works-eda)\n\n<span style=\"font-family:Times New Roman; font-size:18px;\">Brain Tumor Classification Using Transer Learning Tensor Flow. </span>\n\n[Kaggle Link](https://www.kaggle.com/code/abdulbasitniazi/resnet50-eda-transfer-learning)\n\n<span style=\"font-family:Times New Roman; font-size:18px;\">Efficeint Net Fine Tuning </span>\n\n[Kaggle Link](https://www.kaggle.com/code/abdulbasitniazi/enetb7-explained-98-fine-tuning-eda)\n\n<span style=\"font-family:Times New Roman; font-size:18px;\">ResNet 50 From Scratch Tensor Flow. </span>\n\n[Kaggle Link](https://www.kaggle.com/code/abdulbasitniazi/resnet50fromscratch-eda)\n\n\n\n<span style=\"font-family:Times New Roman; font-size:18px;\">Making Note Book Beautiful </span>\n\n[Kaggle Link](https://www.kaggle.com/code/abdulbasitniazi/making-notebook-beautiful)\n\n\n<span style=\"font-family:Times New Roman; font-size:18px;\">Video Explanation </span>\n[Youtube Link](https://www.youtube.com/channel/UCSAw-QDHdXjrAMpg7-TELVA)\n\n","metadata":{}}]}